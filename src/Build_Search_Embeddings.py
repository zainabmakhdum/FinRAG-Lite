# -*- coding: utf-8 -*-
"""AML_Final Project4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f0vcdsCvaHrtmj4PmU3vTMpZnlfJn921
"""

#!pip install -q sentence-transformers faiss-cpu numpy

from google.colab import files
uploaded = files.upload()  # select your 10 json files
json_paths = list(uploaded.keys())
print("Uploaded files:", json_paths)

import json
import re
from typing import List, Dict, Any, Tuple
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# Settings
MODEL_NAME = "BAAI/bge-small-en"
BATCH_SIZE = 16
NORMALIZE_EMBEDDINGS = True

CHUNK_SIZE_CHARS = 1000
CHUNK_OVERLAP_CHARS = 200

# Output files
EMB_PATH = "chunk_embeddings.npy"
CHUNKS_PATH = "chunks.json"
META_PATH = "chunk_meta.json"
FAISS_INDEX_PATH = "faiss.index"

# Text utils
def clean_text(s: str) -> str:
    if not s:
        return ""
    s = s.replace("\u00ad", "")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def chunk_text(text: str, chunk_size: int, overlap: int) -> List[Dict[str, Any]]:

    text = clean_text(text)
    if not text:
        return []

    chunks = []
    start = 0
    n = len(text)

    while start < n:
        end = min(start + chunk_size, n)
        chunk = text[start:end].strip()

        if chunk:
            chunks.append({"text": chunk, "char_start": start, "char_end": end})

        if end == n:
            break
        start = max(0, end - overlap)

    return chunks

# Loading JSONs

def load_docs_from_json(path: str) -> List[Dict[str, Any]]:
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data if isinstance(data, list) else [data]


all_chunks_display: List[str] = []
all_chunks_embed: List[str] = []
chunk_meta: List[Dict[str, Any]] = []

total_pages = 0

for jp in json_paths:
    docs = load_docs_from_json(jp)

    for doc in docs:
        doc_filename = doc.get("filename") or jp

        pages = doc.get("pages", [])
        for p in pages:
            total_pages += 1
            page_number = p.get("page_number", None)
            page_text = p.get("text", "")

            page_chunks = chunk_text(page_text, CHUNK_SIZE_CHARS, CHUNK_OVERLAP_CHARS)

            for i, ch in enumerate(page_chunks):
                display_text = ch["text"]

                embed_text = f"[FILE: {doc_filename}] [PAGE: {page_number}] {display_text}"

                all_chunks_display.append(display_text)
                all_chunks_embed.append(embed_text)

                chunk_meta.append({
                    "source_json": jp,
                    "filename": doc_filename,
                    "page_number": page_number,
                    "chunk_in_page": i,
                    "char_start": ch["char_start"],
                    "char_end": ch["char_end"],
                    "text_len": len(display_text),
                })

print(f"Loaded {len(json_paths)} JSON files")
print(f"Total pages processed: {total_pages}")
print(f"Total chunks created: {len(all_chunks_display)}")
print("Example meta:", chunk_meta[0] if chunk_meta else None)
print("Example chunk preview:", (all_chunks_display[0][:250] + "...") if all_chunks_display else None)


# Embedding

model = SentenceTransformer(MODEL_NAME)

chunk_embeddings = model.encode(
    all_chunks_embed,
    batch_size=BATCH_SIZE,
    show_progress_bar=True,
    normalize_embeddings=NORMALIZE_EMBEDDINGS,
).astype(np.float32)

print("Embeddings shape:", chunk_embeddings.shape)

# Saving artifacts

np.save(EMB_PATH, chunk_embeddings)
print("Saved:", EMB_PATH)

with open(CHUNKS_PATH, "w", encoding="utf-8") as f:
    json.dump(all_chunks_display, f, ensure_ascii=False, indent=2)
print("Saved:", CHUNKS_PATH)

with open(META_PATH, "w", encoding="utf-8") as f:
    json.dump(chunk_meta, f, ensure_ascii=False, indent=2)
print("Saved:", META_PATH)

# Build FAISS index

dim = chunk_embeddings.shape[1]
index = faiss.IndexFlatIP(dim)
index.add(chunk_embeddings)

faiss.write_index(index, FAISS_INDEX_PATH)
print("Saved:", FAISS_INDEX_PATH)

import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# Loading artifacts

chunk_embeddings = np.load("chunk_embeddings.npy")

with open("chunks.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

with open("chunk_meta.json", "r", encoding="utf-8") as f:
    chunk_meta = json.load(f)

index = faiss.read_index("faiss.index")

model = SentenceTransformer("BAAI/bge-small-en")

print("Embeddings:", chunk_embeddings.shape)
print("Chunks:", len(chunks))
print("Meta:", len(chunk_meta))

# Query helper

def search(query, top_k=5):
    q_emb = model.encode(
        [query],
        normalize_embeddings=True
    ).astype(np.float32)

    D, I = index.search(q_emb, top_k)

    print(f"\nQuery: {query}\n")
    for rank, (score, idx) in enumerate(zip(D[0], I[0]), start=1):
        meta = chunk_meta[idx]
        print(f"--- Result {rank} ---")
        print(f"Score: {score:.4f}")
        print(f"File: {meta['filename']} | Page: {meta['page_number']}")
        print(chunks[idx][:300], "...\n")

# Test query
search("What is revenue growth in Intel?", top_k=5)

search("What is revenue growth in Nvidia?", top_k=5)

search("What is revenue growth in TSMC?", top_k=5)